{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\jksls\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jksls\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygeohash in c:\\users\\jksls\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygeohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, franchise_id: int, franchise_name: string, restaurant_franchise_id: int, country: string, city: string, lat: double, lng: double]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark ETL Task\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_restaurant = spark.read.csv('data/restaurant/part-00000-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv', header=True, inferSchema=True)\n",
    "# df_weather = spark.read.csv('data/geo/8/day=01/.part-00037-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet.crc', header=True, inferSchema=True)\n",
    "df_restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def read_data(spark, path, file_type):\n",
    "    \"\"\"\n",
    "    Reads all files of the specified type from all subdirectories within the provided path \n",
    "    and concatenates them into a single DataFrame. It recursively searches every subdirectory,\n",
    "    no matter the depth, for files ending in '.snappy.parquet' if 'parquet' is specified as file_type.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session instance.\n",
    "        path (str): The root directory path that contains the subdirectories with the files.\n",
    "        file_type (str): The type of files to read ('csv' or 'parquet').\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing all data from the files found with the specified extension.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If an unsupported file type is specified or no files are found.\n",
    "    \"\"\"\n",
    "    if file_type not in ['csv', 'parquet']:\n",
    "        raise ValueError(\"Unsupported file type. Please use 'csv' or 'parquet'.\")\n",
    "\n",
    "    all_files = []\n",
    "    # Walk through directory\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            # Check for file extension\n",
    "            if file_type == 'csv' and file.endswith(\".csv\"):\n",
    "                all_files.append(os.path.join(subdir, file))\n",
    "            elif file_type == 'parquet' and file.endswith(\".snappy.parquet\"):\n",
    "                all_files.append(os.path.join(subdir, file))\n",
    "\n",
    "    # Read all files into a DataFrame\n",
    "    if all_files:\n",
    "        if file_type == 'csv':\n",
    "            df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(all_files)\n",
    "        elif file_type == 'parquet':\n",
    "            df = spark.read.parquet(all_files)\n",
    "    else:\n",
    "        # Provide a schema or handle the empty dataset case differently\n",
    "        empty_schema = StructType([])  # Define an empty schema\n",
    "        df = spark.createDataFrame([], schema=empty_schema)  # Create an empty DataFrame with a defined schema\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|          city|   lat|    lng|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|197568495625|          10|    The Golden Spoon|                  24784|     US|       Decatur|34.578|-87.021|\n",
      "| 17179869242|          59|         Azalea Cafe|                  10902|     FR|         Paris|48.861|  2.368|\n",
      "|214748364826|          27|     The Corner Cafe|                  92040|     US|    Rapid City| 44.08|-103.25|\n",
      "|154618822706|          51|        The Pizzeria|                  41484|     AT|        Vienna|48.213| 16.413|\n",
      "|163208757312|          65|       Chef's Corner|                  96638|     GB|        London|51.495| -0.191|\n",
      "| 68719476763|          28|    The Spicy Pickle|                  77517|     US|      Grayling|44.657|-84.744|\n",
      "|223338299419|          28|    The Spicy Pickle|                  36937|     US|        Oswego|43.452|-76.532|\n",
      "|240518168650|          75|     Greenhouse Cafe|                  93164|     NL|     Amsterdam| 52.37|  4.897|\n",
      "|128849018936|          57|The Yellow Submarine|                   5679|     FR|         Paris|48.872|  2.335|\n",
      "|197568495635|          20|       The Brasserie|                  24784|     US|Jeffersonville|39.616|-83.612|\n",
      "| 68719476768|          33|   The Blue Elephant|                  77517|     IT|         Milan|45.479|  9.146|\n",
      "| 51539607582|          31|           Bistro 42|                   6934|     IT|         Milan|45.444|  9.153|\n",
      "| 94489280554|          43|      The Food House|                  95399|     FR|         Paris|48.867|  2.329|\n",
      "|206158430215|           8|     The Green Olive|                  53370|     US|   Haltom City|32.789| -97.28|\n",
      "|154618822657|           2|        Bella Cucina|                  41484|     US|   Fort Pierce|27.412|-80.391|\n",
      "| 17179869217|          34|     The Tasty Treat|                  10902|     US|     Green Bay|44.476|-88.077|\n",
      "|  8589934633|          42|     The Daily Scoop|                  12630|     FR|         Paris|48.854|  2.343|\n",
      "|240518168596|          21|      The Lazy Daisy|                  93164|     US|    Mendenhall| 39.86|-75.646|\n",
      "|171798691906|          67|  Crimson and Clover|                  65939|     NL|     Amsterdam|52.361|  4.894|\n",
      "| 42949673022|          63|          Cafe Paris|                  89646|     GB|        London|51.508| -0.107|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate data\n",
    "dataframe = read_data(spark, 'data/restaurant/', 'csv')\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o89.parquet.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\r\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1(SessionState.scala:110)\r\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1$adapted(SessionState.scala:110)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\r\n\tat org.apache.spark.sql.internal.SessionState.newHadoopConfWithOptions(SessionState.scala:110)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.newHadoopConfiguration(DataSource.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:365)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read and concatenate data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataframe_par \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/geo/8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Show the first few rows of the DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataframe_par\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(spark, path, file_type)\u001b[0m\n\u001b[0;32m     37\u001b[0m         df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(all_files)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Provide a schema or handle the empty dataset case differently\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     empty_schema \u001b[38;5;241m=\u001b[39m StructType([])  \u001b[38;5;66;03m# Define an empty schema\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    542\u001b[0m )\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.parquet.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\r\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1(SessionState.scala:110)\r\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1$adapted(SessionState.scala:110)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\r\n\tat org.apache.spark.sql.internal.SessionState.newHadoopConfWithOptions(SessionState.scala:110)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.newHadoopConfiguration(DataSource.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:365)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate data\n",
    "dataframe_par = read_data(spark, 'data/geo/', 'parquet')\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "dataframe_par.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/geo/august\\day=01\\part-00037-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=01\\part-00038-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=01\\part-00215-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=02\\part-00010-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=02\\part-00011-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=02\\part-00206-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=03\\part-00085-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=03\\part-00086-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=03\\part-00204-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=04\\part-00127-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=04\\part-00128-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=04\\part-00195-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=05\\part-00167-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=05\\part-00168-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=05\\part-00193-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=06\\part-00043-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=06\\part-00044-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=06\\part-00196-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=07\\part-00051-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=07\\part-00052-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=07\\part-00195-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=08\\part-00093-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=08\\part-00094-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=08\\part-00205-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=09\\part-00148-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=09\\part-00149-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=09\\part-00214-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=10\\part-00019-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=10\\part-00020-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=10\\part-00216-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=11\\part-00063-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=11\\part-00064-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=11\\part-00214-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=12\\part-00091-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=12\\part-00092-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=12\\part-00193-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=13\\part-00059-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=13\\part-00060-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=13\\part-00199-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=14\\part-00117-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=14\\part-00118-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=14\\part-00200-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=15\\part-00159-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=15\\part-00160-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=15\\part-00216-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=16\\part-00029-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=16\\part-00030-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=16\\part-00201-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=17\\part-00045-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=17\\part-00046-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=17\\part-00204-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=18\\part-00078-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=18\\part-00079-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=18\\part-00209-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=19\\part-00023-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=19\\part-00024-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=19\\part-00201-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=20\\part-00099-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=20\\part-00100-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=20\\part-00220-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=21\\part-00025-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=21\\part-00026-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=21\\part-00222-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=22\\part-00087-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=22\\part-00177-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=22\\part-00221-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=23\\part-00131-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=23\\part-00183-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=23\\part-00202-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=24\\part-00065-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=24\\part-00180-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=24\\part-00208-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=25\\part-00107-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=25\\part-00108-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=25\\part-00208-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=26\\part-00154-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=26\\part-00179-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=26\\part-00218-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=27\\part-00012-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=27\\part-00181-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=27\\part-00221-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=28\\part-00080-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=28\\part-00178-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=28\\part-00222-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=29\\part-00088-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=29\\part-00182-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=29\\part-00210-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=30\\part-00027-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=30\\part-00028-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=30\\part-00197-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=31\\part-00157-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=31\\part-00158-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/august\\day=31\\part-00188-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=01\\part-00140-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=01\\part-00141-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=01\\part-00229-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=02\\part-00002-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=02\\part-00003-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=02\\part-00228-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=03\\part-00097-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=03\\part-00098-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=03\\part-00225-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=04\\part-00070-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=04\\part-00071-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=04\\part-00213-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=05\\part-00021-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=05\\part-00022-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=05\\part-00205-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=06\\part-00163-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=06\\part-00164-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=06\\part-00210-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=07\\part-00103-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=07\\part-00104-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=07\\part-00219-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=08\\part-00031-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=08\\part-00032-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=08\\part-00207-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=09\\part-00161-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=09\\part-00162-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=09\\part-00225-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=10\\part-00169-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=10\\part-00170-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=10\\part-00224-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=11\\part-00035-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=11\\part-00036-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=11\\part-00228-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=12\\part-00074-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=12\\part-00075-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=12\\part-00224-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=13\\part-00111-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=13\\part-00112-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=13\\part-00219-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=14\\part-00146-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=14\\part-00147-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=14\\part-00197-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=15\\part-00004-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=15\\part-00005-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=15\\part-00203-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=16\\part-00057-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=16\\part-00058-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=16\\part-00199-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=17\\part-00105-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=17\\part-00106-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=17\\part-00207-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=18\\part-00015-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=18\\part-00016-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=18\\part-00212-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=19\\part-00144-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=19\\part-00145-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=19\\part-00211-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=20\\part-00134-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=20\\part-00135-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=20\\part-00220-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=21\\part-00175-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=21\\part-00176-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=21\\part-00226-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=22\\part-00041-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=22\\part-00042-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=22\\part-00223-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=23\\part-00066-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=23\\part-00067-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=23\\part-00226-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=24\\part-00125-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=24\\part-00126-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=24\\part-00223-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=25\\part-00138-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=25\\part-00139-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=25\\part-00213-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=26\\part-00008-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=26\\part-00009-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=26\\part-00217-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=27\\part-00055-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=27\\part-00056-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=27\\part-00217-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=28\\part-00109-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=28\\part-00110-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=28\\part-00227-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=29\\part-00017-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=29\\part-00018-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=29\\part-00229-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=30\\part-00076-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=30\\part-00077-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=30\\part-00227-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=31\\part-00136-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=31\\part-00137-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/october\\day=31\\part-00218-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=01\\part-00155-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=01\\part-00156-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=01\\part-00191-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=02\\part-00123-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=02\\part-00124-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=02\\part-00203-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=03\\part-00083-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=03\\part-00084-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=03\\part-00212-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=04\\part-00039-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=04\\part-00040-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=04\\part-00188-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=05\\part-00165-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=05\\part-00166-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=05\\part-00189-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=06\\part-00089-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=06\\part-00090-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=06\\part-00198-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=07\\part-00049-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=07\\part-00050-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=07\\part-00215-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=08\\part-00101-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=08\\part-00102-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=08\\part-00209-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=09\\part-00150-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=09\\part-00151-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=09\\part-00196-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=10\\part-00142-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=10\\part-00143-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=10\\part-00202-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=11\\part-00081-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=11\\part-00082-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=11\\part-00194-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=12\\part-00033-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=12\\part-00034-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=12\\part-00211-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=13\\part-00173-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=13\\part-00174-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=13\\part-00200-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=14\\part-00121-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=14\\part-00122-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=14\\part-00194-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=15\\part-00047-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=15\\part-00048-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=15\\part-00191-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=16\\part-00006-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=16\\part-00007-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=16\\part-00198-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=17\\part-00119-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=17\\part-00120-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=17\\part-00206-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=18\\part-00068-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=18\\part-00069-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=18\\part-00192-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=19\\part-00113-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=19\\part-00114-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=19\\part-00185-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=20\\part-00115-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=20\\part-00116-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=20\\part-00186-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=21\\part-00152-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=21\\part-00153-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=21\\part-00184-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=22\\part-00061-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=22\\part-00062-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=22\\part-00184-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=23\\part-00013-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=23\\part-00014-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=23\\part-00185-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=24\\part-00171-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=24\\part-00172-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=24\\part-00187-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=25\\part-00132-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=25\\part-00133-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=25\\part-00187-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=26\\part-00053-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=26\\part-00054-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=26\\part-00190-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=27\\part-00000-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=27\\part-00001-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=27\\part-00192-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=28\\part-00129-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=28\\part-00130-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=28\\part-00190-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=29\\part-00095-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=29\\part-00096-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=29\\part-00189-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=30\\part-00072-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=30\\part-00073-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n",
      "data/geo/september\\day=30\\part-00186-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        for file in files:\n",
    "            if file.endswith('.snappy.parquet'):\n",
    "                print(os.path.join(root, file))\n",
    "\n",
    "list_files('data/geo/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\jksls\\AppData\\Local\\Temp\\ipykernel_8368\\18468970.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df = spark.read.parquet('data/geo/august\\day=01\\part-00037-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet')\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('data/geo/august\\day=01\\part-00037-44bd3411-fbe4-4e16-b667-7ec0fc3ad489.c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+----------+----------+\n",
      "|     lng|     lat|avg_tmpr_f|avg_tmpr_c| wthr_date|\n",
      "+--------+--------+----------+----------+----------+\n",
      "|-27.6141|-59.7956|       9.8|     -12.3|2017-08-01|\n",
      "|-27.4095|-59.7956|       9.9|     -12.3|2017-08-01|\n",
      "| -27.205|-59.7956|       9.9|     -12.3|2017-08-01|\n",
      "|-27.0004|-59.7956|      10.0|     -12.2|2017-08-01|\n",
      "|-26.7959|-59.7956|      10.0|     -12.2|2017-08-01|\n",
      "|-27.8186|-59.5911|       9.8|     -12.3|2017-08-01|\n",
      "|-27.6141|-59.5911|      10.0|     -12.2|2017-08-01|\n",
      "|-27.4095|-59.5911|      10.1|     -12.2|2017-08-01|\n",
      "| -27.205|-59.5911|      10.2|     -12.1|2017-08-01|\n",
      "|-27.0004|-59.5911|      10.3|     -12.1|2017-08-01|\n",
      "|-26.7959|-59.5911|      10.4|     -12.0|2017-08-01|\n",
      "|-27.8186|-59.3867|       9.9|     -12.3|2017-08-01|\n",
      "|-27.6141|-59.3867|      10.1|     -12.2|2017-08-01|\n",
      "|-27.4095|-59.3867|      10.3|     -12.1|2017-08-01|\n",
      "| -27.205|-59.3867|      10.5|     -11.9|2017-08-01|\n",
      "|-27.0004|-59.3867|      10.7|     -11.8|2017-08-01|\n",
      "|-26.7959|-59.3867|      10.8|     -11.8|2017-08-01|\n",
      "|-26.5913|-59.3867|      10.9|     -11.7|2017-08-01|\n",
      "|-26.3868|-59.3867|      11.0|     -11.7|2017-08-01|\n",
      "|-26.1822|-59.3867|      11.0|     -11.7|2017-08-01|\n",
      "+--------+--------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '16a2553fc34b4e64b6e4da5c5183d521'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_coordinates(api_key, address):\n",
    "    url = f\"https://api.opencagedata.com/geocode/v1/json?q={address}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data['results'][0]['geometry']['lat'], data['results'][0]['geometry']['lng']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import pygeohash as pgh\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Geolocation and Join Task\").getOrCreate()\n",
    "\n",
    "# Assuming df_restaurant and df_weather are your DataFrames\n",
    "# Define a function to generate a geohash\n",
    "def geohash(lat, lng):\n",
    "    if lat is not None and lng is not None:\n",
    "        return pgh.encode(lat, lng, precision=4)\n",
    "    return None\n",
    "\n",
    "# Register UDF for geohash generation\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "geohash_udf = udf(geohash, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[lng: double, lat: double, avg_tmpr_f: double, avg_tmpr_c: double, wthr_date: string, geohash: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"geohash\", geohash_udf(col(\"lat\"), col(\"lng\")))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn(\"geohash\", geohash_udf(col(\"lat\"), col(\"lng\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = dataframe.join(df, \"geohash\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o265.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 40) (192.168.0.18 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:26)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:26)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_joined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o265.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 40) (192.168.0.18 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:26)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:26)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "print(df_joined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- franchise_id: integer (nullable = true)\n",
      " |-- franchise_name: string (nullable = true)\n",
      " |-- restaurant_franchise_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- avg_tmpr_f: double (nullable = true)\n",
      " |-- avg_tmpr_c: double (nullable = true)\n",
      " |-- wthr_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o265.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 39) (192.168.0.18 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_joined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o265.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 39) (192.168.0.18 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
